{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/jeromemorissard/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/jeromemorissard/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /Users/jeromemorissard/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/jeromemorissard/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.cuda.is_available:False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jeromemorissard/.pyenv/versions/3.10.6/envs/detect_ai_content/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "BertForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device:mps\n",
      "tf.config.experimental.list_physical_devices(GPU):[]\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer, OneHotEncoder\n",
    "\n",
    "from sklearn import set_config; set_config(display='diagram')\n",
    "\n",
    "from detect_ai_content.params import *\n",
    "\n",
    "from detect_ai_content.ml_logic.for_texts.using_ml_features.TrueNetTextLogisticRegression import TrueNetTextLogisticRegression\n",
    "from detect_ai_content.ml_logic.for_texts.using_ml_features.TrueNetTextTfidfNaiveBayesClassifier import TrueNetTextTfidfNaiveBayesClassifier\n",
    "from detect_ai_content.ml_logic.for_texts.using_ml_features.TrueNetTextDecisionTreeClassifier import TrueNetTextDecisionTreeClassifier\n",
    "from detect_ai_content.ml_logic.for_texts.using_ml_features.TrueNetTextSVC import TrueNetTextSVC\n",
    "from detect_ai_content.ml_logic.for_texts.using_ml_features.TrueNetTextKNeighborsClassifier import TrueNetTextKNeighborsClassifier\n",
    "from detect_ai_content.ml_logic.for_texts.using_ml_features.TrueNetTextUsingBERTMaskedPredictions import TrueNetTextUsingBERTMaskedPredictions\n",
    "from detect_ai_content.ml_logic.data import get_enriched_df\n",
    "\n",
    "from detect_ai_content.utils import timer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_enriched_df(purpose=\"test\")\n",
    "y_test = df['generated']\n",
    "\n",
    "TrueNetTextTfidfNaiveBayesClassifier_model = TrueNetTextTfidfNaiveBayesClassifier().local_trained_pipeline()\n",
    "TrueNetTextLogisticRegression_model = TrueNetTextLogisticRegression().local_trained_pipeline()\n",
    "TrueNetTextDecisionTreeClassifier_model = TrueNetTextDecisionTreeClassifier().local_trained_pipeline()\n",
    "TrueNetTextKNeighborsClassifier_model = TrueNetTextKNeighborsClassifier().local_trained_pipeline()\n",
    "TrueNetTextSVC_model = TrueNetTextSVC().local_trained_pipeline()\n",
    "TrueNetTextUsingBERTMaskedPredictions_model = TrueNetTextUsingBERTMaskedPredictions().local_trained_pipeline()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Time spent: 0.03\n",
      "WARNING:root:Time spent: 0.04\n",
      "WARNING:root:Time spent: 0.03\n",
      "WARNING:root:Time spent: 0.03\n",
      "WARNING:root:Time spent: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text\n",
      "0  Last year, I decided that I wanted to take a r...\n",
      "TrueNetTextLogisticRegression_duration: 0.030354583024745807\n",
      "TrueNetTextDecisionTreeClassifier_duration: 0.03990708399214782\n",
      "TrueNetTextKNeighborsClassifier_duration: 0.030976165988249704\n",
      "TrueNetTextSVC_duration: 0.028724415984470397\n",
      "TrueNetTextTfidfNaiveBayesClassifier_duration: 0.0010592500038910657\n",
      "enrich_text_BERT_predictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Time spent: 4.09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "TrueNetTextUsingBERTMaskedPredictions_duration: 4.092468874994665\n"
     ]
    }
   ],
   "source": [
    "from codetiming import Timer\n",
    "import logging\n",
    "\n",
    "# Make one full prediction !\n",
    "text_df = pd.DataFrame(data=df[['text']].loc[0].values, columns=[\"text\"])\n",
    "print(text_df)\n",
    "\n",
    "t = Timer(\"example\", text=\"Time spent: {:.2f}\", logger=logging.warning)\n",
    "t.start()\n",
    "TrueNetTextLogisticRegression_preds = TrueNetTextLogisticRegression_model.predict(text_df)\n",
    "TrueNetTextLogisticRegression_duration = t.stop()\n",
    "print(f\"TrueNetTextLogisticRegression_duration: {TrueNetTextLogisticRegression_duration}\")\n",
    "\n",
    "t.start()\n",
    "TrueNetTextDecisionTreeClassifier_preds = TrueNetTextDecisionTreeClassifier_model.predict(text_df)\n",
    "TrueNetTextDecisionTreeClassifier_duration = t.stop()\n",
    "print(f\"TrueNetTextDecisionTreeClassifier_duration: {TrueNetTextDecisionTreeClassifier_duration}\")\n",
    "\n",
    "t.start()\n",
    "TrueNetTextKNeighborsClassifier_preds = TrueNetTextKNeighborsClassifier_model.predict(text_df)\n",
    "TrueNetTextKNeighborsClassifier_duration = t.stop()\n",
    "print(f\"TrueNetTextKNeighborsClassifier_duration: {TrueNetTextKNeighborsClassifier_duration}\")\n",
    "\n",
    "t.start()\n",
    "TrueNetTextSVC_preds = TrueNetTextSVC_model.predict(text_df)\n",
    "TrueNetTextSVC_duration = t.stop()\n",
    "print(f\"TrueNetTextSVC_duration: {TrueNetTextSVC_duration}\")\n",
    "\n",
    "t.start()\n",
    "TrueNetTextTfidfNaiveBayesClassifier_preds = TrueNetTextTfidfNaiveBayesClassifier_model.predict(text_df)\n",
    "TrueNetTextTfidfNaiveBayesClassifier_duration = t.stop()\n",
    "print(f\"TrueNetTextTfidfNaiveBayesClassifier_duration: {TrueNetTextTfidfNaiveBayesClassifier_duration}\")\n",
    "\n",
    "t.start()\n",
    "TrueNetTextUsingBERTMaskedPredictions_preds = TrueNetTextUsingBERTMaskedPredictions_model.predict(text_df)\n",
    "TrueNetTextUsingBERTMaskedPredictions_duration = t.stop()\n",
    "print(f\"TrueNetTextUsingBERTMaskedPredictions_duration: {TrueNetTextUsingBERTMaskedPredictions_duration}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Unnamed: 0.2  Unnamed: 0.1  Unnamed: 0  \\\n",
      "0            86348         86348      304713   \n",
      "1            24765         24765      105379   \n",
      "2            50237         50237      512698   \n",
      "3             3304          3304      901161   \n",
      "4           103843        103843      322130   \n",
      "...            ...           ...         ...   \n",
      "3995          3960          3960       96948   \n",
      "3996         32306         32306      447554   \n",
      "3997          5352          5352      992163   \n",
      "3998        105984        105984      448358   \n",
      "3999         80051         80051      312078   \n",
      "\n",
      "                                                   text  generated  \\\n",
      "0     Last year, I decided that I wanted to take a r...        1.0   \n",
      "1     A five-day school week provides a more consist...        1.0   \n",
      "2     That means it is a good idea to ask experts at...        0.0   \n",
      "3     For students who need extra help, online class...        1.0   \n",
      "4     An added perk of taking classes online is that...        1.0   \n",
      "...                                                 ...        ...   \n",
      "3995  \"The Challenge of Exploring Venus.\" The author...        0.0   \n",
      "3996  During the summer they can learn important val...        0.0   \n",
      "3997  . In addition, research has also found that cu...        1.0   \n",
      "3998  But it not as bad as you think, hears why.\\n\\n...        0.0   \n",
      "3999  so he change to materials and process to learn...        0.0   \n",
      "\n",
      "                                                 source  punctuations_nb  \\\n",
      "0     huggingface.co_human_ai_generated_text/model_t...               31   \n",
      "1     huggingface.co_human_ai_generated_text/model_t...               20   \n",
      "2     huggingface.co_human_ai_generated_text/model_t...               56   \n",
      "3     huggingface.co_human_ai_generated_text/model_t...               35   \n",
      "4     huggingface.co_human_ai_generated_text/model_t...                8   \n",
      "...                                                 ...              ...   \n",
      "3995     kaggle-ai-generated-vs-human-text/AI_Human.csv               38   \n",
      "3996  huggingface.co_human_ai_generated_text/model_t...               63   \n",
      "3997  huggingface.co_human_ai_generated_text/model_t...               12   \n",
      "3998  huggingface.co_human_ai_generated_text/model_t...               49   \n",
      "3999  huggingface.co_human_ai_generated_text/model_t...               22   \n",
      "\n",
      "      neg_sentiment_polarity  pos_sentiment_polarity  text_corrections_nb  \\\n",
      "0                  -0.066667                2.312424                    6   \n",
      "1                  -0.041667                0.770833                    6   \n",
      "2                  -0.625926                5.114551                   13   \n",
      "3                  -0.500000                2.697478                    4   \n",
      "4                  -0.100000                1.327348                    0   \n",
      "...                      ...                     ...                  ...   \n",
      "3995               -2.703333                1.208333                   13   \n",
      "3996               -2.119583                5.217262                   29   \n",
      "3997               -0.700000                0.900000                    2   \n",
      "3998               -3.404167                0.600000                    7   \n",
      "3999               -0.209659                2.336501                    4   \n",
      "\n",
      "      ...  repetitions_ratio  punctuations_ratio  text_corrections_ratio  \\\n",
      "0     ...           0.057229            0.023343                0.004518   \n",
      "1     ...           0.031414            0.020942                0.006283   \n",
      "2     ...           0.052920            0.017032                0.003954   \n",
      "3     ...           0.034958            0.017233                0.001969   \n",
      "4     ...           0.029900            0.013289                0.000000   \n",
      "...   ...                ...                 ...                     ...   \n",
      "3995  ...           0.054417            0.024328                0.008323   \n",
      "3996  ...           0.060033            0.017270                0.007950   \n",
      "3997  ...           0.038653            0.014963                0.002494   \n",
      "3998  ...           0.090665            0.026288                0.003755   \n",
      "3999  ...           0.072385            0.010907                0.001983   \n",
      "\n",
      "      text_corrections_set_ratio  average_neg_sentiment_polarity  \\\n",
      "0                       0.461538                       -0.000050   \n",
      "1                       1.000000                       -0.000044   \n",
      "2                       0.541667                       -0.000190   \n",
      "3                       0.307692                       -0.000246   \n",
      "4                       0.000000                       -0.000166   \n",
      "...                          ...                             ...   \n",
      "3995                    0.722222                       -0.001731   \n",
      "3996                    0.935484                       -0.000581   \n",
      "3997                    0.285714                       -0.000873   \n",
      "3998                    0.368421                       -0.001826   \n",
      "3999                    0.285714                       -0.000104   \n",
      "\n",
      "      average_pos_sentiment_polarity  average_sentence_lenght  \\\n",
      "0                           0.001741               102.153846   \n",
      "1                           0.000807               159.166667   \n",
      "2                           0.001556               137.000000   \n",
      "3                           0.001328               156.230769   \n",
      "4                           0.002205               120.400000   \n",
      "...                              ...                      ...   \n",
      "3995                        0.000774                86.777778   \n",
      "3996                        0.001430               117.677419   \n",
      "3997                        0.001122               114.571429   \n",
      "3998                        0.000322                98.105263   \n",
      "3999                        0.001158               144.071429   \n",
      "\n",
      "      number_of_tests  number_of_correct_prediction  \\\n",
      "0                  56                            39   \n",
      "1                  59                            35   \n",
      "2                 166                            90   \n",
      "3                 135                            86   \n",
      "4                  40                            31   \n",
      "...               ...                           ...   \n",
      "3995               74                            26   \n",
      "3996              191                            97   \n",
      "3997               48                            26   \n",
      "3998               64                            19   \n",
      "3999               94                            37   \n",
      "\n",
      "      pourcentage_of_correct_prediction  \n",
      "0                                    70  \n",
      "1                                    59  \n",
      "2                                    54  \n",
      "3                                    64  \n",
      "4                                    78  \n",
      "...                                 ...  \n",
      "3995                                 35  \n",
      "3996                                 51  \n",
      "3997                                 54  \n",
      "3998                                 30  \n",
      "3999                                 39  \n",
      "\n",
      "[4000 rows x 23 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enrich_text_BERT_predictions\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m TrueNetTextSVC_preds \u001b[38;5;241m=\u001b[39m TrueNetTextSVC_model\u001b[38;5;241m.\u001b[39mpredict(df)\n\u001b[1;32m      5\u001b[0m TrueNetTextTfidfNaiveBayesClassifier_preds \u001b[38;5;241m=\u001b[39m TrueNetTextTfidfNaiveBayesClassifier_model\u001b[38;5;241m.\u001b[39mpredict(df)\n\u001b[0;32m----> 6\u001b[0m TrueNetTextUsingBERTMaskedPredictions_preds \u001b[38;5;241m=\u001b[39m \u001b[43mTrueNetTextUsingBERTMaskedPredictions_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/detect_ai_content/lib/python3.10/site-packages/sklearn/pipeline.py:600\u001b[0m, in \u001b[0;36mPipeline.predict\u001b[0;34m(self, X, **params)\u001b[0m\n\u001b[1;32m    598\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _routing_enabled():\n\u001b[1;32m    599\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _, name, transform \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter(with_final\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m--> 600\u001b[0m         Xt \u001b[38;5;241m=\u001b[39m \u001b[43mtransform\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    601\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mpredict(Xt, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[1;32m    603\u001b[0m \u001b[38;5;66;03m# metadata routing enabled\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/detect_ai_content/lib/python3.10/site-packages/sklearn/utils/_set_output.py:316\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 316\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    317\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    318\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    319\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    320\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    321\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    322\u001b[0m         )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/detect_ai_content/lib/python3.10/site-packages/sklearn/preprocessing/_function_transformer.py:252\u001b[0m, in \u001b[0;36mFunctionTransformer.transform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Transform X using the forward function.\u001b[39;00m\n\u001b[1;32m    239\u001b[0m \n\u001b[1;32m    240\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;124;03m    Transformed input.\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    251\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_input(X, reset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m--> 252\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkw_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkw_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    253\u001b[0m output_config \u001b[38;5;241m=\u001b[39m _get_output_config(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransform\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdense\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(out, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_names_out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    256\u001b[0m     \u001b[38;5;66;03m# check the consistency between the column provided by `transform` and\u001b[39;00m\n\u001b[1;32m    257\u001b[0m     \u001b[38;5;66;03m# the the column names provided by `get_feature_names_out`.\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/detect_ai_content/lib/python3.10/site-packages/sklearn/preprocessing/_function_transformer.py:379\u001b[0m, in \u001b[0;36mFunctionTransformer._transform\u001b[0;34m(self, X, func, kw_args)\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m func \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    377\u001b[0m     func \u001b[38;5;241m=\u001b[39m _identity\n\u001b[0;32m--> 379\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkw_args\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkw_args\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/DataScience/detect_ai_content/detect_ai_content/ml_logic/for_texts/using_ml_features/TrueNetTextUsingBERTMaskedPredictions.py:150\u001b[0m, in \u001b[0;36msmartBertEnrichFunction\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;124;03m    Create features if they don't exist\u001b[39;00m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    149\u001b[0m data_processed \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m--> 150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpourcentage_of_correct_prediction\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m data_processed:\n\u001b[1;32m    151\u001b[0m     data_processed \u001b[38;5;241m=\u001b[39m enrich_text_BERT_predictions(data_processed)\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data_processed\n",
      "File \u001b[0;32m~/Desktop/DataScience/detect_ai_content/detect_ai_content/ml_logic/for_texts/using_ml_features/TrueNetTextUsingBERTMaskedPredictions.py:244\u001b[0m, in \u001b[0;36menrich_text_BERT_predictions\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (index, row) \u001b[38;5;129;01min\u001b[39;00m data_enriched\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[1;32m    243\u001b[0m     text \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m--> 244\u001b[0m     index_sum \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    245\u001b[0m     (number_of_test, number_of_correct_prediction) \u001b[38;5;241m=\u001b[39m compute_masked_words_BERT_prediction(text)\n\u001b[1;32m    246\u001b[0m     pourcentage \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/Desktop/DataScience/detect_ai_content/detect_ai_content/ml_logic/for_texts/using_ml_features/TrueNetTextUsingBERTMaskedPredictions.py:199\u001b[0m, in \u001b[0;36mcompute_masked_words_BERT_prediction\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m    197\u001b[0m top_clean \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m\n\u001b[1;32m    198\u001b[0m input_ids, mask_idx \u001b[38;5;241m=\u001b[39m BERT_encode(bert_tokenizer, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmasked_sentence\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 199\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m    200\u001b[0m     predict \u001b[38;5;241m=\u001b[39m bert_model(input_ids)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    201\u001b[0m predict_words \u001b[38;5;241m=\u001b[39m BERT_decode(bert_tokenizer, predict[\u001b[38;5;241m0\u001b[39m, mask_idx, :]\u001b[38;5;241m.\u001b[39mtopk(top_k)\u001b[38;5;241m.\u001b[39mindices\u001b[38;5;241m.\u001b[39mtolist(), top_clean)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/detect_ai_content/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/detect_ai_content/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/detect_ai_content/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:1464\u001b[0m, in \u001b[0;36mBertForMaskedLM.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1455\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1456\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1457\u001b[0m \u001b[38;5;124;03m    Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,\u001b[39;00m\n\u001b[1;32m   1458\u001b[0m \u001b[38;5;124;03m    config.vocab_size]` (see `input_ids` docstring) Tokens with indices set to `-100` are ignored (masked), the\u001b[39;00m\n\u001b[1;32m   1459\u001b[0m \u001b[38;5;124;03m    loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[1;32m   1460\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1462\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1464\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1465\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1466\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1467\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1468\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1469\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1470\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1471\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1472\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1473\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1474\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1475\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1476\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1478\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1479\u001b[0m prediction_scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcls(sequence_output)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/detect_ai_content/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/detect_ai_content/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/detect_ai_content/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:1142\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1135\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[1;32m   1136\u001b[0m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[1;32m   1137\u001b[0m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[1;32m   1138\u001b[0m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[1;32m   1139\u001b[0m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[1;32m   1140\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m-> 1142\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1143\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1145\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1146\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1148\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1149\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1150\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1151\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1152\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1153\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1154\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1155\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/detect_ai_content/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/detect_ai_content/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/detect_ai_content/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:695\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    684\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    685\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    686\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    692\u001b[0m         output_attentions,\n\u001b[1;32m    693\u001b[0m     )\n\u001b[1;32m    694\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 695\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    699\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    700\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    701\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    703\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    706\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/detect_ai_content/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/detect_ai_content/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/detect_ai_content/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:585\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    574\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    575\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    582\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m    583\u001b[0m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[1;32m    584\u001b[0m     self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 585\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    586\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    587\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    588\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    589\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    590\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    592\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    594\u001b[0m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/detect_ai_content/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/detect_ai_content/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/detect_ai_content/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:515\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    505\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    506\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    507\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    513\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    514\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m--> 515\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    516\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    517\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    519\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    520\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    521\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    524\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(self_outputs[\u001b[38;5;241m0\u001b[39m], hidden_states)\n\u001b[1;32m    525\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/detect_ai_content/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/detect_ai_content/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/detect_ai_content/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:440\u001b[0m, in \u001b[0;36mBertSdpaSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;66;03m# We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\u001b[39;00m\n\u001b[1;32m    433\u001b[0m \u001b[38;5;66;03m# in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\u001b[39;00m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;66;03m# The tgt_len > 1 is necessary to match with AttentionMaskConverter.to_causal_4d that does not create\u001b[39;00m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;66;03m# a causal mask in case tgt_len == 1.\u001b[39;00m\n\u001b[1;32m    436\u001b[0m is_causal \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    437\u001b[0m     \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_decoder \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_cross_attention \u001b[38;5;129;01mand\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m tgt_len \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    438\u001b[0m )\n\u001b[0;32m--> 440\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaled_dot_product_attention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    441\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery_layer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    442\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_layer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    443\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalue_layer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    444\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    445\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropout_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout_prob\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    446\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    447\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    449\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    450\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mreshape(bsz, tgt_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_head_size)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(df)\n",
    "TrueNetTextUsingBERTMaskedPredictions_preds = TrueNetTextUsingBERTMaskedPredictions_model.predict(df)\n",
    "TrueNetTextLogisticRegression_preds = TrueNetTextLogisticRegression_model.predict(df)\n",
    "TrueNetTextDecisionTreeClassifier_preds = TrueNetTextDecisionTreeClassifier_model.predict(df)\n",
    "TrueNetTextKNeighborsClassifier_preds = TrueNetTextKNeighborsClassifier_model.predict(df)\n",
    "TrueNetTextSVC_preds = TrueNetTextSVC_model.predict(df)\n",
    "TrueNetTextTfidfNaiveBayesClassifier_preds = TrueNetTextTfidfNaiveBayesClassifier_model.predict(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LogisticRegression</th>\n",
       "      <th>DecisionTreeClassifier</th>\n",
       "      <th>KNeighborsClassifier</th>\n",
       "      <th>SVC</th>\n",
       "      <th>BERTMaskedPredictions</th>\n",
       "      <th>TfidfNaiveBayesClassifier</th>\n",
       "      <th>y_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   LogisticRegression  DecisionTreeClassifier  KNeighborsClassifier  SVC  \\\n",
       "0                 1.0                     0.0                   0.0  1.0   \n",
       "1                 1.0                     1.0                   0.0  1.0   \n",
       "\n",
       "   BERTMaskedPredictions  TfidfNaiveBayesClassifier  y_test  \n",
       "0                    1.0                        0.0     0.0  \n",
       "1                    1.0                        1.0     1.0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_df = pd.DataFrame(data={\n",
    "    'LogisticRegression':TrueNetTextLogisticRegression_preds,\n",
    "    'DecisionTreeClassifier':TrueNetTextDecisionTreeClassifier_preds,\n",
    "    'KNeighborsClassifier':TrueNetTextKNeighborsClassifier_preds,\n",
    "    'SVC':TrueNetTextSVC_preds,\n",
    "    'BERTMaskedPredictions':TrueNetTextUsingBERTMaskedPredictions_preds,\n",
    "    'TfidfNaiveBayesClassifier': TrueNetTextTfidfNaiveBayesClassifier_preds,\n",
    "    'y_test':y_test\n",
    "})\n",
    "predictions_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LogisticRegression</th>\n",
       "      <th>DecisionTreeClassifier</th>\n",
       "      <th>KNeighborsClassifier</th>\n",
       "      <th>SVC</th>\n",
       "      <th>BERTMaskedPredictions</th>\n",
       "      <th>TfidfNaiveBayesClassifier</th>\n",
       "      <th>y_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>341</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>342</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>346 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     LogisticRegression  DecisionTreeClassifier  KNeighborsClassifier  SVC  \\\n",
       "0                   1.0                     0.0                   0.0  1.0   \n",
       "1                   1.0                     1.0                   0.0  1.0   \n",
       "2                   1.0                     1.0                   1.0  1.0   \n",
       "3                   0.0                     0.0                   0.0  0.0   \n",
       "4                   0.0                     0.0                   0.0  0.0   \n",
       "..                  ...                     ...                   ...  ...   \n",
       "341                 1.0                     1.0                   1.0  1.0   \n",
       "342                 0.0                     0.0                   0.0  0.0   \n",
       "343                 1.0                     1.0                   1.0  1.0   \n",
       "344                 0.0                     0.0                   0.0  0.0   \n",
       "345                 1.0                     1.0                   1.0  1.0   \n",
       "\n",
       "     BERTMaskedPredictions  TfidfNaiveBayesClassifier  y_test  \n",
       "0                      1.0                        0.0     0.0  \n",
       "1                      1.0                        1.0     1.0  \n",
       "2                      1.0                        1.0     1.0  \n",
       "3                      0.0                        0.0     0.0  \n",
       "4                      0.0                        0.0     0.0  \n",
       "..                     ...                        ...     ...  \n",
       "341                    1.0                        1.0     1.0  \n",
       "342                    1.0                        0.0     0.0  \n",
       "343                    1.0                        1.0     1.0  \n",
       "344                    1.0                        0.0     0.0  \n",
       "345                    1.0                        1.0     1.0  \n",
       "\n",
       "[346 rows x 7 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df['LogisticRegression_diff'] = predictions_df['LogisticRegression'] - predictions_df['y_test']\n",
    "predictions_df['DecisionTreeClassifier_diff'] = predictions_df['DecisionTreeClassifier'] - predictions_df['y_test']\n",
    "predictions_df['KNeighborsClassifier_diff'] = predictions_df['KNeighborsClassifier'] - predictions_df['y_test']\n",
    "predictions_df['SVC_diff'] = predictions_df['SVC'] - predictions_df['y_test']\n",
    "predictions_df['BERTMaskedPredictions_diff'] = predictions_df['BERTMaskedPredictions'] - predictions_df['y_test']\n",
    "predictions_df['TfidfNaiveBayesClassifier_diff'] = predictions_df['TfidfNaiveBayesClassifier'] - predictions_df['y_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df['sum_diff'] = abs(predictions_df['LogisticRegression_diff']) + abs(predictions_df['DecisionTreeClassifier_diff']) + abs(predictions_df['KNeighborsClassifier_diff']) + abs(predictions_df['SVC_diff']) + abs(predictions_df['BERTMaskedPredictions_diff']) + abs(predictions_df['TfidfNaiveBayesClassifier_diff'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.histplot(data=predictions_df, x='sum_diff');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8439306358381503"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(predictions_df['sum_diff'] < 2) / predictions_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LogisticRegression</th>\n",
       "      <th>DecisionTreeClassifier</th>\n",
       "      <th>KNeighborsClassifier</th>\n",
       "      <th>SVC</th>\n",
       "      <th>BERTMaskedPredictions</th>\n",
       "      <th>TfidfNaiveBayesClassifier</th>\n",
       "      <th>y_test</th>\n",
       "      <th>sum_diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>334</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     LogisticRegression  DecisionTreeClassifier  KNeighborsClassifier  SVC  \\\n",
       "275                 0.0                     0.0                   0.0  0.0   \n",
       "229                 0.0                     0.0                   0.0  0.0   \n",
       "66                  0.0                     0.0                   0.0  0.0   \n",
       "213                 0.0                     0.0                   0.0  0.0   \n",
       "118                 0.0                     0.0                   0.0  0.0   \n",
       "192                 1.0                     1.0                   1.0  1.0   \n",
       "149                 1.0                     1.0                   1.0  1.0   \n",
       "132                 1.0                     1.0                   1.0  1.0   \n",
       "294                 1.0                     1.0                   1.0  1.0   \n",
       "282                 0.0                     0.0                   0.0  0.0   \n",
       "255                 1.0                     1.0                   1.0  1.0   \n",
       "244                 0.0                     0.0                   0.0  0.0   \n",
       "227                 1.0                     1.0                   1.0  1.0   \n",
       "226                 1.0                     1.0                   1.0  1.0   \n",
       "218                 0.0                     0.0                   0.0  0.0   \n",
       "334                 1.0                     1.0                   1.0  1.0   \n",
       "63                  0.0                     0.0                   0.0  0.0   \n",
       "121                 0.0                     0.0                   0.0  0.0   \n",
       "69                  0.0                     0.0                   0.0  0.0   \n",
       "5                   1.0                     1.0                   1.0  0.0   \n",
       "160                 0.0                     1.0                   0.0  0.0   \n",
       "184                 0.0                     1.0                   0.0  1.0   \n",
       "101                 0.0                     1.0                   0.0  0.0   \n",
       "92                  0.0                     1.0                   0.0  0.0   \n",
       "90                  1.0                     0.0                   1.0  1.0   \n",
       "61                  1.0                     0.0                   0.0  0.0   \n",
       "305                 0.0                     0.0                   0.0  0.0   \n",
       "41                  0.0                     0.0                   1.0  0.0   \n",
       "32                  1.0                     0.0                   0.0  1.0   \n",
       "317                 1.0                     0.0                   0.0  0.0   \n",
       "16                  1.0                     0.0                   0.0  1.0   \n",
       "135                 1.0                     1.0                   0.0  0.0   \n",
       "257                 1.0                     0.0                   0.0  1.0   \n",
       "53                  1.0                     1.0                   0.0  1.0   \n",
       "148                 0.0                     0.0                   1.0  1.0   \n",
       "0                   1.0                     0.0                   0.0  1.0   \n",
       "\n",
       "     BERTMaskedPredictions  TfidfNaiveBayesClassifier  y_test  sum_diff  \n",
       "275                    0.0                        0.0     1.0       6.0  \n",
       "229                    0.0                        0.0     1.0       6.0  \n",
       "66                     0.0                        0.0     1.0       6.0  \n",
       "213                    0.0                        0.0     1.0       6.0  \n",
       "118                    0.0                        0.0     1.0       6.0  \n",
       "192                    1.0                        1.0     0.0       6.0  \n",
       "149                    1.0                        0.0     0.0       5.0  \n",
       "132                    1.0                        0.0     0.0       5.0  \n",
       "294                    1.0                        0.0     0.0       5.0  \n",
       "282                    0.0                        1.0     1.0       5.0  \n",
       "255                    1.0                        0.0     0.0       5.0  \n",
       "244                    1.0                        0.0     1.0       5.0  \n",
       "227                    1.0                        0.0     0.0       5.0  \n",
       "226                    1.0                        0.0     0.0       5.0  \n",
       "218                    0.0                        1.0     1.0       5.0  \n",
       "334                    0.0                        1.0     0.0       5.0  \n",
       "63                     0.0                        1.0     1.0       5.0  \n",
       "121                    0.0                        1.0     1.0       5.0  \n",
       "69                     0.0                        1.0     1.0       5.0  \n",
       "5                      1.0                        0.0     0.0       4.0  \n",
       "160                    0.0                        1.0     1.0       4.0  \n",
       "184                    0.0                        0.0     1.0       4.0  \n",
       "101                    0.0                        1.0     1.0       4.0  \n",
       "92                     1.0                        0.0     1.0       4.0  \n",
       "90                     1.0                        0.0     0.0       4.0  \n",
       "61                     0.0                        1.0     1.0       4.0  \n",
       "305                    1.0                        1.0     1.0       4.0  \n",
       "41                     0.0                        1.0     1.0       4.0  \n",
       "32                     1.0                        0.0     0.0       3.0  \n",
       "317                    1.0                        1.0     1.0       3.0  \n",
       "16                     1.0                        0.0     0.0       3.0  \n",
       "135                    1.0                        0.0     0.0       3.0  \n",
       "257                    1.0                        0.0     1.0       3.0  \n",
       "53                     0.0                        0.0     0.0       3.0  \n",
       "148                    1.0                        0.0     0.0       3.0  \n",
       "0                      1.0                        0.0     0.0       3.0  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_df[predictions_df['sum_diff'] > 2][['LogisticRegression', 'DecisionTreeClassifier', 'KNeighborsClassifier', 'SVC','BERTMaskedPredictions','TfidfNaiveBayesClassifier', 'y_test', 'sum_diff']].sort_values(by=['sum_diff'], ascending=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "detect_ai_content",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
